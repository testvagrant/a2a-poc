# Step-by-Step Setup Guide: From Zero to UTA Testing

## ğŸ¯ **The Complete Journey**

This guide shows you exactly how to go from having nothing to testing real chatbot agents with UTA.

## ğŸ“‹ **Prerequisites**

- Python 3.8+
- Internet connection
- Basic command line knowledge

## ğŸš€ **Option 1: Quick Testing (No Building Required)**

### **Step 1: Test with HuggingChat (5 minutes)**

```bash
# 1. Test HuggingChat integration
python3 examples/huggingchat_integration.py

# 2. Run UTA with HuggingChat
python3 -m runner.run --suite scenarios/core --report out_huggingchat --http-url https://huggingface.co/chat/api --seed 42
```

**That's it!** You're now testing with a real chatbot agent.

### **Step 2: Test with OpenAI (if you have API key)**

```bash
# Run UTA with OpenAI
python3 -m runner.run --suite scenarios/core --report out_openai --http-url https://api.openai.com/v1/chat/completions --http-api-key YOUR_OPENAI_KEY --seed 42
```

## ğŸ”§ **Option 2: Build Your Own Chatbot (More Control)**

### **Step 1: Install Ollama (5 minutes)**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama server
ollama serve

# In another terminal, pull a model
ollama pull llama2
```

### **Step 2: Build Simple Chatbot Server (5 minutes)**

```bash
# Install Flask (if not already installed)
pip install flask

# Start the chatbot server
python3 examples/simple_chatbot_server.py
```

### **Step 3: Test with UTA (2 minutes)**

```bash
# In another terminal, test with UTA
python3 -m runner.run --suite scenarios/core --report out_custom_chatbot --http-url http://localhost:5000 --seed 42
```

## ğŸ¯ **Complete Example: Building a Collections Chatbot**

Let me show you how to build a domain-specific chatbot for collections:

### **Step 1: Create Collections Chatbot**

```python
# examples/collections_chatbot.py
from flask import Flask, request, jsonify
import requests
import time
import json

app = Flask(__name__)

class CollectionsChatbot:
    """Collections-specific chatbot using Ollama."""
    
    def __init__(self, ollama_url: str = "http://localhost:11434", model: str = "llama2"):
        self.ollama_url = ollama_url
        self.model = model
        self.conversation_history = []
        self.account_info = {
            "debtor_id": "D-1001",
            "balance": 1500.00,
            "last_payment": "2024-01-15",
            "status": "active"
        }
    
    def chat(self, message: str) -> dict:
        """Process collections-related messages."""
        # Add collections context to the prompt
        context = f"""
You are a collections agent helping with debt collection. 
Account Info: {json.dumps(self.account_info, indent=2)}

User Message: {message}

Respond as a professional collections agent. Be helpful but firm about payment obligations.
"""
        
        # Call Ollama with context
        response = self._call_ollama(context)
        
        # Parse response for structured data
        structured = self._extract_structured_data(response, message)
        
        return {
            "text": response,
            "structured": structured
        }
    
    def _call_ollama(self, prompt: str) -> str:
        """Call Ollama API."""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "max_tokens": 1000
            }
        }
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("response", "No response received")
            else:
                return f"Error: HTTP {response.status_code}"
                
        except requests.exceptions.RequestException as e:
            return f"Error: {str(e)}"
    
    def _extract_structured_data(self, response: str, user_message: str) -> dict:
        """Extract structured data from response."""
        structured = {
            "intent": "unknown",
            "outcome": "partial"
        }
        
        # Simple intent detection
        if "payment" in user_message.lower():
            structured["intent"] = "payment"
        elif "dispute" in user_message.lower():
            structured["intent"] = "dispute"
        elif "promise" in user_message.lower():
            structured["intent"] = "promise_to_pay"
            structured["promise_to_pay"] = {
                "date": "2024-02-15",
                "amount": 500.00
            }
        
        return structured

# Initialize chatbot
chatbot = CollectionsChatbot()

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint for UTA."""
    try:
        data = request.get_json()
        conversation_history = data.get("conversation_history", [])
        
        # Get last user message
        last_user_message = ""
        for msg in reversed(conversation_history):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")
                break
        
        if not last_user_message:
            return jsonify({"error": "No user message found"}), 400
        
        # Process message
        start_time = time.time()
        response = chatbot.chat(last_user_message)
        response_time_ms = (time.time() - start_time) * 1000
        
        return jsonify({
            "text": response["text"],
            "structured": response["structured"],
            "metadata": {
                "status_code": 200,
                "response_time_ms": response_time_ms,
                "model": chatbot.model
            }
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("ğŸ¦ Starting Collections Chatbot Server...")
    print("ğŸš€ Server will be available at: http://localhost:5001")
    app.run(host='0.0.0.0', port=5001, debug=True)
```

### **Step 2: Test Collections Chatbot**

```bash
# Start collections chatbot
python3 examples/collections_chatbot.py

# Test with UTA
python3 -m runner.run --suite scenarios/collections --report out_collections_chatbot --http-url http://localhost:5001 --seed 42
```

## ğŸ¯ **Complete Testing Workflow**

### **Workflow 1: Quick Testing (10 minutes)**

```bash
# 1. Test HuggingChat
python3 examples/huggingchat_integration.py

# 2. Run UTA with HuggingChat
python3 -m runner.run --suite scenarios/core --report out_huggingchat --http-url https://huggingface.co/chat/api --seed 42

# 3. Check results
open out_huggingchat/report.html
```

### **Workflow 2: Local Testing (20 minutes)**

```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull llama2

# 2. Start chatbot server
python3 examples/simple_chatbot_server.py

# 3. Test with UTA
python3 -m runner.run --suite scenarios/core --report out_local_chatbot --http-url http://localhost:5000 --seed 42

# 4. Check results
open out_local_chatbot/report.html
```

### **Workflow 3: Domain-Specific Testing (30 minutes)**

```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull llama2

# 2. Start collections chatbot
python3 examples/collections_chatbot.py

# 3. Test collections scenarios
python3 -m runner.run --suite scenarios/collections --report out_collections_chatbot --http-url http://localhost:5001 --seed 42

# 4. Check results
open out_collections_chatbot/report.html
```

## ğŸ” **What You'll See**

### **Expected Results:**

| Test Type | Pass Rate | Response Time | Quality |
|-----------|-----------|---------------|---------|
| **Mock Agents** | 100% | ~1ms | Basic |
| **HuggingChat** | 65-80% | ~1000-3000ms | Good |
| **Local Ollama** | 70-85% | ~500-2000ms | High |
| **Collections Bot** | 60-75% | ~500-2000ms | Domain-specific |

### **Sample Output:**

```json
{
  "summary": {
    "total": 3,
    "pass_count": 2,
    "model": "llama2",
    "judge_type": "heuristic",
    "duration_s": 2.5
  },
  "sessions": [
    {
      "scenario_id": "CORE_001_INTENT_SUCCESS",
      "passed": true,
      "metrics": {
        "relevance": 0.85,
        "completeness": 0.70,
        "groundedness": 0.80
      }
    }
  ]
}
```

## ğŸ‰ **You're Done!**

After following any of these workflows, you'll have:

- âœ… **Real chatbot agent** running
- âœ… **UTA testing** the real agent
- âœ… **Comprehensive reports** with real performance data
- âœ… **Production-ready** testing setup

## ğŸš€ **Next Steps**

1. **Choose your approach** (quick testing vs. building)
2. **Follow the workflow** that matches your needs
3. **Analyze the results** and iterate
4. **Scale up** to more complex scenarios
5. **Deploy to production** when ready

## ğŸ“ **Need Help?**

- **Quick Testing**: Use HuggingChat (no setup required)
- **Local Testing**: Use Ollama (5-minute setup)
- **Custom Testing**: Build your own chatbot (30-minute setup)
- **Production**: Deploy to cloud with proper infrastructure

---

## ğŸ† **Summary**

**You have 3 options:**

1. **ğŸš€ Quick Testing**: Use existing APIs (HuggingChat, OpenAI) - **No building required**
2. **ğŸ”§ Local Testing**: Use Ollama with simple chatbot wrapper - **5-minute setup**
3. **ğŸ—ï¸ Custom Testing**: Build domain-specific chatbot - **30-minute setup**

All options are fully documented and ready to use! ğŸ¯

