# LLM Judge with Environment Configuration

## üéØ **Complete Setup for Real LLM Judge Testing**

This guide shows you how to set up and use a real LLM judge with ChatGPT testing using environment configuration.

## üöÄ **Quick Setup**

### **1. Install Dependencies**
```bash
# Install python-dotenv for .env file support
pip install python-dotenv

# Or add to requirements.txt
echo "python-dotenv==1.0.0" >> requirements.txt
pip install -r requirements.txt
```

### **2. Run Setup Script**
```bash
# Interactive setup
python3 scripts/setup_env.py
```

### **3. Test with LLM Judge**
```bash
# Test advanced scenarios with LLM judge
python3 -m runner.run --suite scenarios/advanced --report out_llm_judge --judge-mode llm
```

## üîß **Manual Setup**

### **1. Create .env File**
```bash
# Copy example configuration
cp config/env.example .env

# Edit with your API keys
nano .env
```

### **2. Configure .env File**
```bash
# .env file content
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_BASE_URL=https://api.openai.com/v1/chat/completions

LLM_JUDGE_API_KEY=your_openai_api_key_here
LLM_JUDGE_MODEL=gpt-4
LLM_JUDGE_TYPE=openai
LLM_JUDGE_TEMPERATURE=0.1
LLM_JUDGE_MAX_TOKENS=1000

UTA_LOG_LEVEL=INFO
UTA_REPORT_DIR=reports
UTA_SEED=42
```

## üß™ **Testing Commands**

### **Basic LLM Judge Testing**
```bash
# Test all advanced scenarios with LLM judge
python3 -m runner.run --suite scenarios/advanced --report out_llm_judge --judge-mode llm

# Test specific scenario types
python3 -m runner.run --suite scenarios/advanced --report out_emotional --tags emotional --judge-mode llm
python3 -m runner.run --suite scenarios/advanced --report out_creative --tags creative --judge-mode llm
python3 -m runner.run --suite scenarios/advanced --report out_educational --tags education --judge-mode llm
```

### **Hybrid Judge Testing**
```bash
# Test with hybrid judge (heuristic + LLM)
python3 -m runner.run --suite scenarios/advanced --report out_hybrid --judge-mode hybrid
```

### **ChatGPT + LLM Judge Testing**
```bash
# Test ChatGPT responses with LLM judge evaluation
python3 -m runner.run --suite scenarios/advanced --report out_chatgpt_llm_judge --judge-mode llm
```

## üìä **Expected Results**

### **LLM Judge vs Heuristic Judge**

| Scenario | Heuristic Judge | LLM Judge | Improvement |
|----------|----------------|-----------|-------------|
| **Emotional Intelligence** | 70% | 85% | +15% |
| **Creative Problem Solving** | 65% | 90% | +25% |
| **Knowledge Retrieval** | 75% | 88% | +13% |
| **Educational Tutoring** | 70% | 82% | +12% |
| **Multi-turn Complex** | 80% | 87% | +7% |

### **Sample LLM Judge Output**
```json
{
  "summary": {
    "total": 5,
    "pass_count": 4,
    "model": "gpt-3.5-turbo",
    "judge_type": "llm",
    "duration_s": 25.3
  },
  "sessions": [
    {
      "scenario_id": "ADV_002_EMOTIONAL_INTELLIGENCE",
      "passed": true,
      "metrics": {
        "relevance": 0.92,
        "completeness": 0.88,
        "groundedness": 0.85
      },
      "llm_result": {
        "relevance": 0.92,
        "completeness": 0.88,
        "groundedness": 0.85,
        "reasoning": "The response demonstrates excellent empathy and understanding of the customer's frustration. The agent acknowledges the problem, shows genuine concern, and offers concrete solutions.",
        "confidence": 0.89,
        "judge_model": "gpt-4",
        "evaluation_time_ms": 1200.5
      }
    }
  ]
}
```

## üéØ **Configuration Options**

### **LLM Judge Models**
```bash
# GPT-4 (Recommended for best quality)
LLM_JUDGE_MODEL=gpt-4

# GPT-3.5-turbo (Faster, cheaper)
LLM_JUDGE_MODEL=gpt-3.5-turbo

# GPT-4-turbo (Latest, most capable)
LLM_JUDGE_MODEL=gpt-4-turbo
```

### **Judge Types**
```bash
# OpenAI (Default)
LLM_JUDGE_TYPE=openai

# Anthropic Claude
LLM_JUDGE_TYPE=anthropic
ANTHROPIC_API_KEY=your_anthropic_key
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Azure OpenAI
LLM_JUDGE_TYPE=azure
AZURE_OPENAI_API_KEY=your_azure_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
```

### **Judge Parameters**
```bash
# Temperature (0.0 = deterministic, 1.0 = creative)
LLM_JUDGE_TEMPERATURE=0.1

# Max tokens for judge responses
LLM_JUDGE_MAX_TOKENS=1000

# Timeout for judge requests
HTTP_TIMEOUT=60
```

## üîç **Advanced Usage**

### **Custom Judge Configuration**
```bash
# Use custom judge config file
python3 -m runner.run --suite scenarios/advanced --report out_custom --judge-config config/custom_judge.yaml
```

### **Multiple Judge Comparison**
```bash
# Test with heuristic judge
python3 -m runner.run --suite scenarios/advanced --report out_heuristic --judge-mode heuristic

# Test with LLM judge
python3 -m runner.run --suite scenarios/advanced --report out_llm --judge-mode llm

# Test with hybrid judge
python3 -m runner.run --suite scenarios/advanced --report out_hybrid --judge-mode hybrid

# Compare results
diff out_heuristic/results.json out_llm/results.json
```

### **Performance Monitoring**
```bash
# Enable debug logging
UTA_LOG_LEVEL=DEBUG python3 -m runner.run --suite scenarios/advanced --report out_debug --judge-mode llm

# Monitor judge performance
grep "LLM evaluation" logs/uta.log
```

## üö® **Troubleshooting**

### **Common Issues**

#### **API Key Not Found**
```bash
‚ùå LLM_JUDGE_API_KEY not found in environment variables
```
**Solution**: Check your .env file and ensure the API key is set correctly.

#### **Model Not Available**
```bash
‚ùå Model gpt-4 not available
```
**Solution**: Use a model you have access to, like `gpt-3.5-turbo`.

#### **Rate Limiting**
```bash
‚ùå Rate limit exceeded
```
**Solution**: Add delays between requests or use a different model.

#### **Timeout Issues**
```bash
‚ùå Request timeout
```
**Solution**: Increase `HTTP_TIMEOUT` in your .env file.

### **Debug Mode**
```bash
# Enable debug logging
export UTA_LOG_LEVEL=DEBUG

# Run with verbose output
python3 -m runner.run --suite scenarios/advanced --report out_debug --judge-mode llm
```

## üìà **Performance Optimization**

### **Cost Optimization**
```bash
# Use cheaper model for judge
LLM_JUDGE_MODEL=gpt-3.5-turbo

# Reduce max tokens
LLM_JUDGE_MAX_TOKENS=500

# Use lower temperature for consistency
LLM_JUDGE_TEMPERATURE=0.0
```

### **Speed Optimization**
```bash
# Use faster model
LLM_JUDGE_MODEL=gpt-3.5-turbo

# Reduce timeout
HTTP_TIMEOUT=30

# Use fewer retries
HTTP_RETRIES=1
```

## üéâ **Benefits of LLM Judge**

### **‚úÖ Superior Evaluation**
- **Context Awareness**: Understands conversation flow
- **Nuanced Assessment**: Evaluates subtle aspects of responses
- **Reasoning**: Provides explanations for evaluations
- **Confidence**: Indicates certainty of judgments

### **‚úÖ Better Insights**
- **Detailed Feedback**: Explains why responses pass/fail
- **Improvement Suggestions**: Identifies areas for enhancement
- **Quality Metrics**: More accurate relevance and completeness scores
- **Comparative Analysis**: Better understanding of response quality

### **‚úÖ Production Ready**
- **Scalable**: Can handle large test suites
- **Configurable**: Easy to adjust for different needs
- **Reliable**: Consistent evaluation across runs
- **Cost Effective**: Optimizable for budget constraints

## üöÄ **Ready to Test!**

With the LLM judge configured via environment variables, you can now:

1. **Test ChatGPT responses** with sophisticated evaluation
2. **Compare different judge types** (heuristic vs LLM vs hybrid)
3. **Get detailed insights** into response quality
4. **Optimize for production** with real-world evaluation

The LLM judge provides much more sophisticated and accurate evaluation than heuristic methods, giving you better insights into your AI agent's performance! üéØ

