"""
Universal Strategy - Domain-Agnostic AI Testing

This strategy tests fundamental AI capabilities that apply to ALL agents,
regardless of domain. It's auto-generated by analyzing the Agent API.
"""

import json
import time
from typing import Dict, Any, Optional, List
from .base_strategy import BaseStrategy

class UniversalStrategy(BaseStrategy):
    """
    Universal Strategy: Tests fundamental AI capabilities across all domains.
    
    This strategy is auto-generated by analyzing the Agent API and tests:
    - Basic conversation flow
    - Intent understanding
    - Error handling
    - Context retention
    - Response quality
    - API compliance
    """
    
    def __init__(self, agent_api_analysis: Dict[str, Any] = None):
        super().__init__(
            name="Universal",
            description="Domain-agnostic strategy for testing fundamental AI capabilities"
        )
        self.agent_api_analysis = agent_api_analysis or {}
        self._conversation_context = []
        self._turn_count = 0
        self._tested_capabilities = set()
        
    def first_message(self, scenario: Dict[str, Any]) -> str:
        """Send the scenario's initial user message."""
        self._conversation_context = []
        self._turn_count = 0
        self._tested_capabilities = set()
        return scenario["conversation"]["initial_user_msg"]
        
    def next_message(self, last_agent_response: Dict[str, Any], scenario: Dict[str, Any]) -> Optional[str]:
        """Generate next message to test fundamental AI capabilities."""
        self._turn_count += 1
        
        # Update conversation context
        self._conversation_context.append({
            'role': 'assistant',
            'content': last_agent_response.get('text', ''),
            'structured': last_agent_response.get('structured', {}),
            'turn': self._turn_count
        })
        
        # Determine which capability to test next
        capability_to_test = self._determine_next_capability_test(scenario, last_agent_response)
        
        # Generate message to test that capability
        next_message = self._generate_capability_test_message(capability_to_test, scenario, last_agent_response)
        
        # Mark capability as tested
        self._tested_capabilities.add(capability_to_test)
        
        # Add to context
        if next_message:
            self._conversation_context.append({
                'role': 'user',
                'content': next_message,
                'turn': self._turn_count + 1
            })
        
        return next_message
        
    def should_continue(self, conversation: List[Dict[str, str]], scenario: Dict[str, Any]) -> bool:
        """Continue until all fundamental capabilities are tested."""
        max_turns = scenario["conversation"].get("max_turns", 5)
        
        if len(conversation) >= max_turns:
            return False
            
        # Check if all fundamental capabilities have been tested
        all_capabilities_tested = len(self._tested_capabilities) >= len(self._get_fundamental_capabilities())
        
        return not all_capabilities_tested
        
    def _determine_next_capability_test(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Determine which fundamental capability to test next."""
        
        fundamental_capabilities = self._get_fundamental_capabilities()
        
        # Find untested capabilities
        untested = [cap for cap in fundamental_capabilities if cap not in self._tested_capabilities]
        
        if untested:
            return untested[0]
        
        # If all tested, return the most important one
        return fundamental_capabilities[0]
        
    def _get_fundamental_capabilities(self) -> List[str]:
        """Get comprehensive list of fundamental AI capabilities to test."""
        return [
            # Core Conversation Capabilities
            'conversation_flow',
            'intent_understanding',
            'context_retention',
            'turn_taking',
            'conversation_repair',
            
            # Language Understanding
            'natural_language_processing',
            'ambiguity_resolution',
            'sarcasm_detection',
            'sentiment_analysis',
            'entity_recognition',
            
            # Response Quality
            'response_relevance',
            'response_completeness',
            'response_accuracy',
            'response_consistency',
            'response_helpfulness',
            
            # Error Handling
            'error_detection',
            'error_recovery',
            'graceful_degradation',
            'fallback_handling',
            'exception_management',
            
            # API & Technical
            'api_compliance',
            'rate_limiting',
            'timeout_handling',
            'authentication',
            'data_validation',
            
            # Reasoning & Logic
            'logical_reasoning',
            'causal_reasoning',
            'analogical_reasoning',
            'deductive_reasoning',
            'inductive_reasoning',
            
            # Memory & Learning
            'short_term_memory',
            'long_term_memory',
            'episodic_memory',
            'semantic_memory',
            'learning_adaptation',
            
            # Safety & Ethics
            'bias_detection',
            'harmful_content_detection',
            'privacy_protection',
            'ethical_guidelines',
            'safety_guardrails',
            
            # Multimodal Capabilities
            'text_processing',
            'image_understanding',
            'audio_processing',
            'multimodal_integration',
            'format_adaptation',
            
            # Performance & Efficiency
            'response_time',
            'resource_usage',
            'scalability',
            'throughput',
            'latency_optimization'
        ]
        
    def _generate_capability_test_message(self, capability: str, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> Optional[str]:
        """Generate message to test a specific capability."""
        
        capability_tests = {
            # Core Conversation Capabilities
            'conversation_flow': self._test_conversation_flow,
            'intent_understanding': self._test_intent_understanding,
            'context_retention': self._test_context_retention,
            'turn_taking': self._test_turn_taking,
            'conversation_repair': self._test_conversation_repair,
            
            # Language Understanding
            'natural_language_processing': self._test_nlp,
            'ambiguity_resolution': self._test_ambiguity_resolution,
            'sarcasm_detection': self._test_sarcasm_detection,
            'sentiment_analysis': self._test_sentiment_analysis,
            'entity_recognition': self._test_entity_recognition,
            
            # Response Quality
            'response_relevance': self._test_response_relevance,
            'response_completeness': self._test_response_completeness,
            'response_accuracy': self._test_response_accuracy,
            'response_consistency': self._test_response_consistency,
            'response_helpfulness': self._test_response_helpfulness,
            
            # Error Handling
            'error_detection': self._test_error_detection,
            'error_recovery': self._test_error_recovery,
            'graceful_degradation': self._test_graceful_degradation,
            'fallback_handling': self._test_fallback_handling,
            'exception_management': self._test_exception_management,
            
            # API & Technical
            'api_compliance': self._test_api_compliance,
            'rate_limiting': self._test_rate_limiting,
            'timeout_handling': self._test_timeout_handling,
            'authentication': self._test_authentication,
            'data_validation': self._test_data_validation,
            
            # Reasoning & Logic
            'logical_reasoning': self._test_logical_reasoning,
            'causal_reasoning': self._test_causal_reasoning,
            'analogical_reasoning': self._test_analogical_reasoning,
            'deductive_reasoning': self._test_deductive_reasoning,
            'inductive_reasoning': self._test_inductive_reasoning,
            
            # Memory & Learning
            'short_term_memory': self._test_short_term_memory,
            'long_term_memory': self._test_long_term_memory,
            'episodic_memory': self._test_episodic_memory,
            'semantic_memory': self._test_semantic_memory,
            'learning_adaptation': self._test_learning_adaptation,
            
            # Safety & Ethics
            'bias_detection': self._test_bias_detection,
            'harmful_content_detection': self._test_harmful_content_detection,
            'privacy_protection': self._test_privacy_protection,
            'ethical_guidelines': self._test_ethical_guidelines,
            'safety_guardrails': self._test_safety_guardrails,
            
            # Multimodal Capabilities
            'text_processing': self._test_text_processing,
            'image_understanding': self._test_image_understanding,
            'audio_processing': self._test_audio_processing,
            'multimodal_integration': self._test_multimodal_integration,
            'format_adaptation': self._test_format_adaptation,
            
            # Performance & Efficiency
            'response_time': self._test_response_time,
            'resource_usage': self._test_resource_usage,
            'scalability': self._test_scalability,
            'throughput': self._test_throughput,
            'latency_optimization': self._test_latency_optimization
        }
        
        test_function = capability_tests.get(capability, self._test_conversation_flow)
        return test_function(scenario, last_response)
        
    def _test_conversation_flow(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test basic conversation flow."""
        if self._turn_count == 1:
            return "Can you help me understand what you can do?"
        elif self._turn_count == 2:
            return "That's helpful. Can you give me more details?"
        else:
            return "Thank you for your help. Is there anything else I should know?"
            
    def _test_intent_understanding(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test intent understanding capabilities."""
        intents = [
            "I need help with something",
            "Can you explain how this works?",
            "I want to know more about your capabilities",
            "What are my options here?"
        ]
        return intents[self._turn_count % len(intents)]
        
    def _test_error_handling(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test error handling capabilities."""
        error_scenarios = [
            "I don't understand what you mean",
            "That doesn't make sense to me",
            "I'm confused about this",
            "Can you clarify what you just said?"
        ]
        return error_scenarios[self._turn_count % len(error_scenarios)]
        
    def _test_context_retention(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test context retention across turns."""
        if self._turn_count == 1:
            return "My name is John and I'm looking for help with my account."
        elif self._turn_count == 2:
            return "Can you remember what I told you about my name?"
        else:
            return "What was the first thing I mentioned to you?"
            
    def _test_response_quality(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response quality and helpfulness."""
        quality_tests = [
            "Can you be more specific?",
            "That's not very helpful",
            "I need a clearer answer",
            "Can you provide more details?"
        ]
        return quality_tests[self._turn_count % len(quality_tests)]
        
    def _test_api_compliance(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test API compliance and structured responses."""
        compliance_tests = [
            "Can you provide a structured response?",
            "I need this in JSON format",
            "Can you give me a summary of our conversation?",
            "What's the status of my request?"
        ]
        return compliance_tests[self._turn_count % len(compliance_tests)]
        
    # Core Conversation Capabilities
    def _test_turn_taking(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test turn-taking in conversation."""
        turn_tests = [
            "Let me ask you something else",
            "Can I interrupt you for a moment?",
            "I have another question",
            "Let me change the topic"
        ]
        return turn_tests[self._turn_count % len(turn_tests)]
        
    def _test_conversation_repair(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test conversation repair capabilities."""
        repair_tests = [
            "I didn't understand that, can you explain?",
            "Can you rephrase that?",
            "I'm confused, can you clarify?",
            "Let me try asking this differently"
        ]
        return repair_tests[self._turn_count % len(repair_tests)]
        
    # Language Understanding
    def _test_nlp(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test natural language processing capabilities."""
        nlp_tests = [
            "Can you understand complex sentences?",
            "What about slang and informal language?",
            "Do you understand different languages?",
            "Can you parse grammar correctly?"
        ]
        return nlp_tests[self._turn_count % len(nlp_tests)]
        
    def _test_ambiguity_resolution(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test ambiguity resolution capabilities."""
        ambiguity_tests = [
            "I saw a man on a hill with a telescope",
            "The chicken is ready to eat",
            "Time flies like an arrow",
            "The old man the boat"
        ]
        return ambiguity_tests[self._turn_count % len(ambiguity_tests)]
        
    def _test_sarcasm_detection(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test sarcasm detection capabilities."""
        sarcasm_tests = [
            "Oh great, another error message",
            "That's just what I needed",
            "Perfect, now it's broken",
            "Thanks for nothing"
        ]
        return sarcasm_tests[self._turn_count % len(sarcasm_tests)]
        
    def _test_sentiment_analysis(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test sentiment analysis capabilities."""
        sentiment_tests = [
            "I'm really frustrated with this",
            "This is amazing, thank you!",
            "I'm not sure how I feel about this",
            "I'm so excited to try this"
        ]
        return sentiment_tests[self._turn_count % len(sentiment_tests)]
        
    def _test_entity_recognition(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test entity recognition capabilities."""
        entity_tests = [
            "I live in New York City",
            "My name is John Smith",
            "I work at Microsoft",
            "My email is john@example.com"
        ]
        return entity_tests[self._turn_count % len(entity_tests)]
        
    # Response Quality
    def _test_response_relevance(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response relevance."""
        relevance_tests = [
            "That doesn't answer my question",
            "You're going off topic",
            "Can you stay focused on what I asked?",
            "This is not what I need"
        ]
        return relevance_tests[self._turn_count % len(relevance_tests)]
        
    def _test_response_completeness(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response completeness."""
        completeness_tests = [
            "That's not a complete answer",
            "You only answered part of my question",
            "I need more details",
            "Can you be more thorough?"
        ]
        return completeness_tests[self._turn_count % len(completeness_tests)]
        
    def _test_response_accuracy(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response accuracy."""
        accuracy_tests = [
            "That doesn't sound right",
            "Are you sure about that?",
            "I think that's incorrect",
            "Can you double-check that information?"
        ]
        return accuracy_tests[self._turn_count % len(accuracy_tests)]
        
    def _test_response_consistency(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response consistency."""
        consistency_tests = [
            "You said something different earlier",
            "That contradicts what you told me before",
            "You're being inconsistent",
            "Which answer is correct?"
        ]
        return consistency_tests[self._turn_count % len(consistency_tests)]
        
    def _test_response_helpfulness(self, scenario: Dict[str, Any], last_response: Dict[str, Any]) -> str:
        """Test response helpfulness."""
        helpfulness_tests = [
            "That's not very helpful",
            "Can you be more useful?",
            "I need practical advice",
            "How does this help me?"
        ]
        return helpfulness_tests[self._turn_count % len(helpfulness_tests)]
        
    def get_strategy_info(self) -> Dict[str, Any]:
        """Get information about this strategy."""
        return {
            'name': self.name,
            'description': self.description,
            'type': 'universal_ai_powered',
            'domain': 'agnostic',
            'capabilities': [
                'conversation_flow_testing',
                'intent_understanding_testing',
                'error_handling_testing',
                'context_retention_testing',
                'response_quality_testing',
                'api_compliance_testing'
            ],
            'tested_capabilities': list(self._tested_capabilities),
            'agent_api_analysis': self.agent_api_analysis
        }
